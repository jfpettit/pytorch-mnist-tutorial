{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-oE0JV_SCIQ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network for MNIST digit recognition in Pytorch."
      ]
    },
    {
      "metadata": {
        "id": "hTZGnnrJDC1W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Some of the required libraries and data"
      ]
    },
    {
      "metadata": {
        "id": "iOv-x-jCCVSJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "(Lots of the code here is modified from a tutorial I found somewhere. I am not trying to steal credit, I simply have completely lost where I initially found the code. I ended up using the tutorial to learn and then modifying it to get the best possible performance from the model. If you read this and realize that your code was what I modified, reach out and I'll edit it to credit you if you so desire. But this is also very generic, typical MNIST tutorial code so maybe most tutorials are just similar to each other.)\n",
        "\n",
        "Additionally, I am still a student myself. I'm here to learn, if you are a knowledgable person who finds some error in my explanation, please don't hesitate to reach out to me or fork this notebook on GitHub, edit it, and submit a merge request with an explanation of what you found wrong, and why your changes make it correct.\n",
        "\n",
        "This first cell is for Google Colab users only, if you are not on Colab you WILL still need to import the torch module, however you will not need any of the other lines in the first cell.\n",
        "\n",
        "The second cell imports some required libraries. Torchvision is a computer vision library in Pytorch, torchvision.transforms allows you to do transformations on images and tensors (tensors AKA N-dimensional arrays). torchvision.datasets is where we will pull the MNIST data that comes packaged with Pytorch from.\n"
      ]
    },
    {
      "metadata": {
        "id": "qKVZhFL1r2b5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X40jeplqsBlZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NCDPU1DwDLd9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this third cell, we are setting the batch size equal to 50 and then loading our data using Pytorch's DataLoader module. The arguments to the DataLoader should be self-explanatory, with the possible exception of 'transform=transforms.ToTensor()'. To clarify, that particular command is simply converting our input images to tensors so that we can feed them to the network. Classes is a tuple of charcters where each character corresponds to one of the classes in our dataset. In this case, MNIST has handwritten digits 0 through 9 in the data."
      ]
    },
    {
      "metadata": {
        "id": "La44hAHItDhN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 5
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "89121e81-6252-4adc-e61e-29663af01300",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520657612680,
          "user_tz": 300,
          "elapsed": 29999,
          "user": {
            "displayName": "Jacob Pettit",
            "photoUrl": "//lh5.googleusercontent.com/-sM8UbM9OHiw/AAAAAAAAAAI/AAAAAAAAAPs/dbzd3qS8zdI/s50-c-k-no/photo.jpg",
            "userId": "104946537452403383669"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 50\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=1000)\n",
        "\n",
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WeT3MgkXD8qL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Defining the network"
      ]
    },
    {
      "metadata": {
        "id": "vEPx_iYlEA4r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we are defining the network within a class. We use 'nn.Module' to tell Pytorch this is going to be a network instance. Following that the init function is where each layer is initialized. To explain some of the functions and their arguments: the call 'nn.Conv2d' takes the following parameters as inputs: (input channels, output channels, kernel size, stride (default is 1), and padding = 2); there are more arguments which can be passed into it, they are found in the Pytorch documentation here: (http://pytorch.org/docs/master/index.html). Following our two convolutional layers are three fully connected layers. Fully connected layers are declared with call to 'nn.Linear' and the arguments given to them here are (input channels, output channels). \n",
        "\n",
        "Now, on to the 'forward' function. This function takes an argument 'x' and passes it through our network. So, here we see that x is the data we are training/testing the network on. The calls to 'F.max_pool2d' are taking the output of a convolutional layer and 'pooling' it to reduce its size by two. The 'F.relu' call within the max pooling call applies the ReLU activation function to the convolutional layer. The argument '2' simply tells the max pooling layer to use a 2 by 2 kernel size. The function 'x.view' is simply what we use to reshape our output from the convolutional layer into a vector for the fully connected layer. The '-1' in the arguments allows the view function to use our batch size to infer what that dimension should be equal to, while $64*7*7$ is the size of our output from the 'conv2' layer. Each subsequent call to 'F.relu' simply applies the ReLU activation function to the layer called within it. The call to 'F.dropout' applies dropout to the 'x' passed into it and the 'training' argument is used to give the function awareness to whether the net is in a training or evaluation mode. Finally, 'F.log_softmax(x)' applies softmax activation function to the output of the final fully connected layer.\n",
        "\n",
        "'net = Net()' simply creates an instance of our class 'Net()', while net.cuda() compiles it to run on the GPU. Comment out the line 'net.cuda()' if you do not have access to a GPU.\n",
        "\n",
        "**Note that before giving output of a convolutional layer to a linear layer, the output from the convolutional layer MUST be reshaped into a vector (hence the $64 * 7 * 7$ weirdness going on next to the defining of 'fc1'). For me, figuring out why $64 * 7 * 7$ was the correct reshaping size took multiple hours and the help of two graduate students, so I'll just tell you here why that is the correct size. In the defining of layer 'conv2', we declare output channels of 64, so that is why the 64 is there. Now, why $7 * 7$? This is because at this point in our neural network, the size of the image has been reduced to 7 by 7. Lets follow the resizing of the image start to finish: Initially we have images of size $28*28$, these go through layer conv1 and get their size reduced by 1/2 in the max pooling layer, so then the size is $14 * 14$. After this resizing, this output again gets size reduced by 1/2 in the second max pooling layer so the size then becomes $7 * 7$, and this is finally the numbers we use in our reshaping of output $-1, 64 * 7 * 7$. In summary: $28 * 28$ --> $14*14$ --> $7 * 7$ **"
      ]
    },
    {
      "metadata": {
        "id": "vzKE02VIuBJn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "df2c90d3-5131-48bb-8306-fc8a2e1264cf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520657624767,
          "user_tz": 300,
          "elapsed": 2643,
          "user": {
            "displayName": "Jacob Pettit",
            "photoUrl": "//lh5.googleusercontent.com/-sM8UbM9OHiw/AAAAAAAAAAI/AAAAAAAAAPs/dbzd3qS8zdI/s50-c-k-no/photo.jpg",
            "userId": "104946537452403383669"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# to get back to original, add padding=2 to both conv layers and for fc1 do 64*7*7\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 150, 5, stride=1, padding=2)\n",
        "    self.conv2 = nn.Conv2d(150, 64, 5, stride=1, padding=2)\n",
        "    self.fc1 = nn.Linear(64*7*7, 1024)\n",
        "    self.fc2 = nn.Linear(1024, 500)\n",
        "    self.fc3 = nn.Linear(500, 10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "    x = x.view(-1, 64*7*7)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = self.fc3(x)\n",
        "    return F.log_softmax(x)\n",
        "\n",
        "net = Net()\n",
        "net.cuda()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d (1, 150, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (conv2): Conv2d (150, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (fc1): Linear(in_features=3136, out_features=1024)\n",
              "  (fc2): Linear(in_features=1024, out_features=500)\n",
              "  (fc3): Linear(in_features=500, out_features=10)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "Hrq9FUyrvJhP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "95f07c23-d14e-49b5-d4d8-76d8d654e525",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520657627964,
          "user_tz": 300,
          "elapsed": 329,
          "user": {
            "displayName": "Jacob Pettit",
            "photoUrl": "//lh5.googleusercontent.com/-sM8UbM9OHiw/AAAAAAAAAAI/AAAAAAAAAPs/dbzd3qS8zdI/s50-c-k-no/photo.jpg",
            "userId": "104946537452403383669"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(net)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d (1, 150, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (conv2): Conv2d (150, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (fc1): Linear(in_features=3136, out_features=1024)\n",
            "  (fc2): Linear(in_features=1024, out_features=500)\n",
            "  (fc3): Linear(in_features=500, out_features=10)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Luk7EF_uKxDe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss, Optimization, and Training"
      ]
    },
    {
      "metadata": {
        "id": "ZFp3DxQYK1lK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So here in the cell directly below, we import the torch optimization library as optim. Following that, the loss function is defined. In this case, the loss function used is Cross Entropy Loss. Then, the optimizer is initialized. Here, I used Stochastic Gradient Descent. It takes in the parameters to the network (given by 'net.parameters()'), the learning rate 'lr', and the momentum '0.95'. I chose 1e-2 for my learning rate because I have seen it work well before, and it seems to be a pretty common choice for a starting learning rate, and I chose 0.95 for my momentum because I saw a tutorial using 0.9 so I played around with different values and seemed to be getting best results with 0.95."
      ]
    },
    {
      "metadata": {
        "id": "bIx0mOArvLbE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
        "optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.95)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JrzMg5OeLlha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we define our training loop. First, we import a function 'Variable'. Variable is part of Pytorch's autograd package, and I have just begun to scratch the surface of autograd so the best place to find information on that is here: (http://pytorch.org/docs/master/notes/autograd.html). Variable is going to allow us to wrap our data in it and perform propagation, differentiation, and other mathematical operations needed for deep learning on them. It's also letting us compile them to go to GPU with the '.cuda()' call. If you don't have a GPU to run this on, you'll need to go below and delete the call to '.cuda()'. train_loss and train_acc are just going to be lists containing our training accuracy and training loss in each iteration, while i = 0 is simply being used as an iteration counter in the for loop. Now, on to the for loop itself. We are running the loop for as many epochs as we would like to do. To change the number of epochs, change the number inside the call to 'range(3)'. The 'for data, target in train_loader: ' is allowing us to step through our training data in batch sizes of 50. The data is the input tensors and the target is the label associated with that input. \n",
        "\n",
        "On the first line, we simply convert our input data and target to Variables and compile them for CUDA (again, delete the call to cuda if you do not have a CUDA enabled GPU to run on). \n",
        "\n",
        "Our second line zeros the gradients for our optimizer.\n",
        "\n",
        "The third line passes our data through the network and generates output. \n",
        "\n",
        "The fourth line calculates the loss function for the output we generated and the target classification. \n",
        "\n",
        "The fifth line performs backpropagation of our loss.\n",
        "\n",
        "The sixth line simply appends our current loss to the train_loss list.\n",
        "\n",
        "The seventh line steps our optimizer in the direction it computed. \n",
        "\n",
        "The eighth, ninth, and tenth lines are all focused around computing our current accuracy and appending it to the train_acc list.\n",
        "\n",
        "The 'if' statement is used to print out training updates every 1000 iterations."
      ]
    },
    {
      "metadata": {
        "id": "J0jYeb5WviLq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 5
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ba5e0c81-0448-4883-a1de-72298e48fb2c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520657710978,
          "user_tz": 300,
          "elapsed": 78314,
          "user": {
            "displayName": "Jacob Pettit",
            "photoUrl": "//lh5.googleusercontent.com/-sM8UbM9OHiw/AAAAAAAAAAI/AAAAAAAAAPs/dbzd3qS8zdI/s50-c-k-no/photo.jpg",
            "userId": "104946537452403383669"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "i = 0\n",
        "for epoch in range(3):\n",
        "  for data, target in train_loader:\n",
        "    data, target = Variable(data.cuda()), Variable(target.cuda())\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(data)\n",
        "    loss = criterion(outputs, target)\n",
        "    loss.backward()\n",
        "    train_loss.append(loss.data[0])\n",
        "    optimizer.step()\n",
        "    prediction = outputs.data.max(1)[1]\n",
        "    accuracy = prediction.eq(target.data).sum()/batch_size*100\n",
        "    train_acc.append(accuracy)\n",
        "    if i % 1000 == 0:\n",
        "      print('Train Step: {}\\tLoss: {:.10f}\\tAccuracy: {:.10f}'.format(i, loss.data[0], accuracy))\n",
        "    i += 1\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Step: 0\tLoss: 2.3147785664\tAccuracy: 6.0000000000\n",
            "Train Step: 1000\tLoss: 0.0469076298\tAccuracy: 98.0000000000\n",
            "Train Step: 2000\tLoss: 0.0813489035\tAccuracy: 98.0000000000\n",
            "Train Step: 3000\tLoss: 0.0053700125\tAccuracy: 100.0000000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iLzm7arVN0eh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A tiny bit of network evaluation!"
      ]
    },
    {
      "metadata": {
        "id": "D6vTiegeN5ok",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we import the library NumPy so that we can use a function from it which will generate n random integers for us. The integers it produces will be used to index into our test data labels and our predictions so that we can compare our target and our actual output while seeing new numbers almost every time. To show more or less numbers at a time, change the third argument to 'np.random.randint'. Currently it shows 10 numbers at a time. Then, we produce an iterator object to step through our test data and output before finally using the print function and some iterator comprehension to step through our testing labels to our random indices and show those labels to us. Almost all code from here down, minus the random indices, has been borrowed from various Pytorch tutorials. Thanks for helping us learn Pytorch!"
      ]
    },
    {
      "metadata": {
        "id": "lgbVvE2qEY5p",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea12aaf6-dede-447f-bb97-84c6550e9e1c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520657829653,
          "user_tz": 300,
          "elapsed": 294,
          "user": {
            "displayName": "Jacob Pettit",
            "photoUrl": "//lh5.googleusercontent.com/-sM8UbM9OHiw/AAAAAAAAAAI/AAAAAAAAAPs/dbzd3qS8zdI/s50-c-k-no/photo.jpg",
            "userId": "104946537452403383669"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "rand_check = np.random.randint(0, 1000, 10)\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in rand_check))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GroundTruth:      1     3     5     5     7     7     9     3     6     0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wvxogdx0Opfk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This cell just generates outputs from our test data for us."
      ]
    },
    {
      "metadata": {
        "id": "WMIKlXG3qpdW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "78f3935c-dcc0-4eff-ad71-33e4d0483c1c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520657830879,
          "user_tz": 300,
          "elapsed": 263,
          "user": {
            "displayName": "Jacob Pettit",
            "photoUrl": "//lh5.googleusercontent.com/-sM8UbM9OHiw/AAAAAAAAAAI/AAAAAAAAAPs/dbzd3qS8zdI/s50-c-k-no/photo.jpg",
            "userId": "104946537452403383669"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "outputs = net(Variable(images).cuda())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "nbBkotaBOtqj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, we step through our outputs produced in the line above and print out what our network believes the digit is. You can compare the printed out predictions to the printed out ground truth above and see if you find any mistakes in your model."
      ]
    },
    {
      "metadata": {
        "id": "giblLHltrZnG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "847b6be0-3cca-4343-cdb6-77ab450263e7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520657832495,
          "user_tz": 300,
          "elapsed": 271,
          "user": {
            "displayName": "Jacob Pettit",
            "photoUrl": "//lh5.googleusercontent.com/-sM8UbM9OHiw/AAAAAAAAAAI/AAAAAAAAAPs/dbzd3qS8zdI/s50-c-k-no/photo.jpg",
            "userId": "104946537452403383669"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "_, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
        "                              for j in rand_check))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted:      1     3     5     5     7     7     9     3     6     0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yfMDQRTqO6yR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, in this cell we simply compute our total test accuracy. We use a for loop to step through our test data, generate outputs, compare them to the ground truth, and then finally compute the fraction of correct classifications and multiply it by 100 to make it a percentage."
      ]
    },
    {
      "metadata": {
        "id": "aDAYmNiE6cfN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5a5d57c9-13ad-468d-a524-9fb5997a9bb8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520657838773,
          "user_tz": 300,
          "elapsed": 2188,
          "user": {
            "displayName": "Jacob Pettit",
            "photoUrl": "//lh5.googleusercontent.com/-sM8UbM9OHiw/AAAAAAAAAAI/AAAAAAAAAPs/dbzd3qS8zdI/s50-c-k-no/photo.jpg",
            "userId": "104946537452403383669"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "output = net(Variable(images).cuda())\n",
        "correct = 0\n",
        "total = 0\n",
        "for data in test_loader:\n",
        "    images, labels = data\n",
        "    outputs = net(Variable(images).cuda())\n",
        "    outputs = outputs.cpu()\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100.0 * correct / total))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 99 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QSO5fwCy7fxv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}